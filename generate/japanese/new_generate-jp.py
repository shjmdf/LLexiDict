import os
import json
import asyncio
import sqlite3
import time
from openai import AsyncOpenAI
import random
import re

# ================= é…ç½® =================
API_KEY = "" 
BASE_URL = ""
MODEL_NAME = ""

CONCURRENCY = 128
SOURCE_FILE = "jp-clean.txt"
DB_NAME = "japanese_dictionary.db"
SYSTEM_MESSAGE_CONTENT = (
    "You are a Japanese dictionary generator. Your response MUST be ONLY the requested JSON object. "
    "DO NOT include any explanatory text, preambles, comments, or chain-of-thought before or after the JSON block. "
    "Start immediately with '{' and end with '}'."
)
# ================= æ•°æ®åº“new =================
async def db_writer(queue):
    def blocking_db_init():
        conn = sqlite3.connect(DB_NAME, check_same_thread=False)
        cursor = conn.cursor()
        cursor.execute('PRAGMA journal_mode=WAL;')
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS dictionary (
                word TEXT PRIMARY KEY,
                keywords TEXT,
                data JSON,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        return conn, cursor

    conn, cursor = await asyncio.to_thread(blocking_db_init)

    batch_buffer = []
    last_commit = time.time()

    while True:
        item = await queue.get()
        if item is None: break
        
        word, keywords, data_str = item
        batch_buffer.append((word, keywords, data_str))

        current_time = time.time()
        if len(batch_buffer) >= 50 or (current_time - last_commit > 2 and batch_buffer):
            try:
                batch_to_write = batch_buffer.copy()
                batch_buffer = []

                def blocking_db_write(batch):
                    cursor.executemany("INSERT OR REPLACE INTO dictionary (word, keywords, data) VALUES (?, ?, ?)", batch)
                    conn.commit()
                
                await asyncio.to_thread(blocking_db_write, batch_to_write)
                last_commit = current_time
                
            except Exception as e:
                print(f"âš ï¸ DB Error: {e}")
                batch_buffer = batch_to_write + batch_buffer
        
        queue.task_done()

    # å¤„ç†å¾ªç¯é€€å‡ºåå‰©ä½™çš„ä»»ä½•é¡¹ç›®
    if batch_buffer:
        try:
            def blocking_db_final_write(batch):
                cursor.executemany("INSERT OR REPLACE INTO dictionary (word, keywords, data) VALUES (?, ?, ?)", batch)
                conn.commit()
            await asyncio.to_thread(blocking_db_final_write, batch_buffer)
        except Exception as e:
            print(f"âš ï¸ Final DB Error: {e}")
    
    await asyncio.to_thread(conn.close)
    print("æ•°æ®åº“å†™å…¥å®Œæˆã€‚")

# ================= æ—¥è¯­ Prompt =================
def get_japanese_prompt(word):
    return f"""
    Role: Meticulous and Verifying Japanese-Chinese Lexicographer.
    Target Word: "{word}" (Japanese).
    Target Audience: Advanced Learners (N1/N2) aiming for native-like nuance.

    **Core Principles: ACCURACY > COMPLETENESS. VERIFICATION is MANDATORY.**

    **ğŸ”¥ğŸ”¥ CRITICAL VERIFICATION STEPS (Must perform before generating): ğŸ”¥ğŸ”¥**

    1.  **AMBIGUITY CHECK:**
        * Does "{word}" have multiple, distinct meanings or parts of speech? (e.g., `ãã†ã„ã†` as a pre-noun adjective vs. `ãã†è¨€ã†` as a verb phrase).
        * **Action:** You MUST select the **single most common/primary meaning** associated with the `word` spelling.
        * **Constraint:** All subsequent fields (`pos`, `grammar_meta`, `inflections_detail`, `senses`, `examples`) MUST align 100% with this SINGLE chosen meaning. **Do not mix meanings.**

    2.  **DATA CONSISTENCY CHECK:**
        * `pitch_accent` ([num]) MUST exactly match the `pitch_visual` (L/H graph).
        * `inflections_detail` (æ´»ç”¨å½¢) MUST logically match the `pos` (è¯æ€§) and `grammar_meta` (è¯­æ³•). (e.g., If `pos` is 'n.', `inflections_detail.forms` MUST be an empty array `[]`).
        * All `examples` MUST use the word in the exact `pos` and `sense` defined above.

    3.  **PRECISION CHECK:**
        * `ruby` (æŒ¯å‡å): Must be 100% accurate, mapping the `kana` reading precisely to the `jp` kanji. **Pay extreme attention to ãŠãã‚ŠãŒãª (okurigana)** (e.g., `ãŠæ›ã‘ã—ã¦` -> `ãŠæ›(ã‹)ã‘ã—ã¦`).
        * `pitch_accent`: Source from standard lexicographical data (e.g., NHK). **If the pitch accent is unknown or widely disputed, use `"[?] Unknown"` instead of guessing.**

    ---
    Output STRICT JSON (No Markdown):
    {{
      "word": "Standard Written Form (e.g. é£Ÿã¹ã‚‹, ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿, è–”è–‡)",
      
      "readings": {{//must be generated
          "kana": "Full Hiragana (e.g. ãŸã¹ã‚‹, ã“ã‚“ã´ã‚…ãƒ¼ãŸ)",
          "katakana": "Full Katakana (e.g. ã‚¿ãƒ™ãƒ«, ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿) - CRITICAL for search",
          "romaji": "Hepburn",
          "pitch_accent": "[num] Type (e.g. [2] Nakadaka) or [?] Unknown",
          "pitch_visual": "Text graph (e.g. LHHLL) or 'N/A'"
      }},

      "pos": "v. (Godan/Ichidan) / adj-i / adj-na / n. / exp. / rentaishi", // **Added 'rentaishi'**
      
      "grammar_meta": {{
          "verb_group": "Godan / Ichidan / Suru / N/A",
          "transitivity": "Transitive (ä»–) / Intransitive (è‡ª) / N/A",
          "paired_verb": "Counterpart (e.g. 'kieru' -> 'kesu') or null"
      }},

      "inflections_detail": {{
          "forms": [
             // "Te-form", "Nai-form", "Ta-form", etc.
             // **If 'pos' is not a verb/adjective, this MUST be []**
          ]
      }},

      "search_keywords": [//must be generated
          "Must include: Kanji form",
          "Must include: Kana reading (Hiragana)",
          "Must include: Katakana reading (e.g. ã‚¿ãƒ™ãƒ«)", 
          "Must include: Romaji",
          "Must include: All generated conjugated forms (if any)"
      ],

      "script_nuance": "Analysis: Is Kanji standard? Is it often written in Katakana for emphasis, slang, or biological naming? (e.g. 'Often written as ãƒã‚³ in scientific contexts')",
      
      "cultural_decoding": {{
          "register": "Teineigo / Kudaketa / Sonkeigo / Kenjougo / Neutral",
          "air_reading": "Hidden nuance / Implication",
          "caution": "Taboo / Usage warning / Common Pitfall (e.g. 'Do not confuse with X')"
      }},

      "senses": [
        {{
          "definitions": {{//be specific and concise, accurate and contextual
              "cn": "Natural Simplified Chinese, with cultural context and explicit usage",
              "jp": "Kokugo Jiten definition, with cultural context and explicit usage",
              "en": "Logical English definition, precise and contextual"
          }},
          "core_image": "Mental picture / Underlying concept",
          "collocations": [
              "Particle Usage (~ni vs ~wo)",
              "Set Phrase / Yojijukugo"
          ],
          "synonym_discrimination": "Compare with similar Kanji/Words",
          "examples": [
            {{
               "jp": "Natural sentence with Kanji",
               "kana": "Full Hiragana reading",
               "ruby": "Kanji(Kana) format (MUST BE 100% ACCURATE)",
               "cn": "Translation"
            }}
          ]
        }}
      ]
    }}
    Final Output Constraint: Your entire response must consist of the complete, valid JSON object, starting with '{{' and ending with '}}'. Nothing else.
    """


# ================= JSON è§£æ=================
def robust_json_parser(raw_content):
    try:
        data = json.loads(raw_content)
        return data, raw_content
    except json.JSONDecodeError as e:
        print(f"âš ï¸ ç›´æ¥è§£æå¤±è´¥: {e.msg}ã€‚å›é€€åˆ°æ­£åˆ™æå–...")
        match = re.search(r'\{.*\}', raw_content.strip(), re.DOTALL)
        
        if not match:
            raise ValueError("JSON_BLOCK_NOT_FOUND: æ— æ³•åœ¨åŸå§‹è¾“å‡ºä¸­éš”ç¦»å®Œæ•´çš„ {} ç»“æ„ã€‚")

        content_json_only = match.group(0)
        
        # 3. æ¸…ç†å°¾éšé€—å·
        content_json_clean = re.sub(r',\s*([\]\}])', r'\1', content_json_only)

        try:
            data = json.loads(content_json_clean)
            return data, content_json_clean
        except json.JSONDecodeError as final_e:
            raise ValueError(f"JSON_PARSE_FAIL (Internal): æ— æ³•è§£ææ¸…ç†åçš„ JSONã€‚Error: {final_e.msg}")


# ================= API Worker =================
async def worker(sem, client, queue, word):
    async with sem:
        last_error = None
        
        for attempt in range(3):
            try:
                response = await asyncio.wait_for(
                    client.chat.completions.create(
                        model=MODEL_NAME,
                        messages=[
                            {"role": "system", "content": SYSTEM_MESSAGE_CONTENT},
                            {"role": "user", "content": get_japanese_prompt(word)}
                        ],
                        response_format={"type": "json_object"},
                        temperature=0.1
                    ),
                    timeout=200
                )
                
                raw_content = response.choices[0].message.content

                data, data_str = robust_json_parser(raw_content)
                
                inflections = data.get("inflections", [])
                if word not in inflections:
                    inflections.append(word)
                keywords_str = " ".join([str(x) for x in inflections]).lower()
                
                await queue.put((word, keywords_str, data_str))
                print(f"âœ… {word}")
                return
                
            except Exception as e:
                last_error = e
                error_str = str(e)
                
                if "429" in error_str:
                    print(f"â³ é™æµç­‰å¾…: {word}")
                    await asyncio.sleep(5 + random.uniform(0, 5)) # å¢åŠ æŠ–åŠ¨
                elif "JSONDecodeError" in error_str or "JSON_BLOCK_NOT_FOUND" in error_str or "JSON_PARSE_FAIL" in error_str:
                    print(f"âš ï¸ JSON ä¸¥é‡é”™è¯¯: {word} | {e}")
                    await asyncio.sleep(1)
                else:
                    print(f"âŒ Worker é”™è¯¯: {word} | {e}")
                    await asyncio.sleep(1 + random.uniform(0, 2))
        
        print(f"âŒ {word} å¤±è´¥ | æœ€ç»ˆåŸå› : {last_error}")
        
# ================= ä¸»ç¨‹åº =================
async def main():
    conn = sqlite3.connect(DB_NAME)
    try:
        existing = set(row[0] for row in conn.execute("SELECT word FROM dictionary"))
    except:
        existing = set()
    conn.close()
    print(f"åº“ä¸­å·²æœ‰ {len(existing)} ä¸ªè¯ã€‚")

    base_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(base_dir, SOURCE_FILE)
    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ° {SOURCE_FILE}ï¼")
        return

    with open(file_path, 'r', encoding='utf-8') as f:
        all_words = [line.strip() for line in f if line.strip()]

    tasks_to_run = [w for w in all_words if w not in existing]
    if not tasks_to_run:
        print("æ•°æ®åº“å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ“ä½œï¼")
        return
        
    print(f"å‰©ä½™ä»»åŠ¡: {len(tasks_to_run)} ä¸ªã€‚ä½¿ç”¨æ¨¡å‹: {MODEL_NAME} | å¹¶å‘: {CONCURRENCY}")

    queue = asyncio.Queue()
    client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)
    sem = asyncio.Semaphore(CONCURRENCY)
    db_task = asyncio.create_task(db_writer(queue))

    print(f"æ­£åœ¨åˆ›å»º {len(tasks_to_run)} ä¸ª API ä»»åŠ¡...")
    workers = [
        asyncio.create_task(worker(sem, client, queue, word)) 
        for word in tasks_to_run
    ]

    print(f"ğŸƒ å¼€å§‹å¤„ç†... (å¹¶å‘ä¸Šé™ {CONCURRENCY})")
    await asyncio.gather(*workers)
    
    print("\nâœ… æ‰€æœ‰ API worker å‡å·²å®Œæˆã€‚")

    print("â³ æ­£åœ¨ç­‰å¾…æ•°æ®åº“é˜Ÿåˆ—æ¸…ç©º...")
    await queue.join()

    print("âš å‘é€å…³é—­ä¿¡å·åˆ°æ•°æ®åº“å†™å…¥çº¿ç¨‹...")
    await queue.put(None)
    await db_task
    
    print("æ—¥è¯­è¯å…¸æ„å»ºå®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())